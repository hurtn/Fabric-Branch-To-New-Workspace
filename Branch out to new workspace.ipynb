{"cells":[{"cell_type":"markdown","source":["##### Branch out to new workspace notebook\n\nThis notebook mimics the [branch out to new workspace functionality](https://blog.fabric.microsoft.com/en-us/blog/introducing-new-branching-capabilities-in-fabric-git-integration) in the Fabric UI.\n\nIn addition to this:\n<ul>\n<li>Default lakehouses are updated to the corresponding \"local\" lakehouse</li>\n<li>Creates shortcuts to tables or shortcuts in the source lakehouse </li>\n<li>Sets lakehouse connections for semantic models to \"local\" lakehouse</li> \n<li>Rebinds reports to \"local\" semantic models</li></ul>\n\nRequirements:\n<ul>\n<li>Source workspace (\"source\") needs to be connected to Azure Devops Git repos</li>\n<li>Target workspace (\"local\") will be recreated using the same capacity</li>\n<li>Azure Devops PAT token required for creating new branch</li>\n<li>Requires Semantic Link Labs installed by pip install below.</li></ul>\n\nLimitations of current script:\n\n<ul>\n<li>Recreates items using git. Please see <a href=https://learn.microsoft.com/en-us/fabric/cicd/git-integration/intro-to-git-integration?tabs=azure-devops#supported-items> git supported items </a></li>\n<li>Does not recreate workspace roles, direct shares or lakehouse data access roles</li>\n<li>Only updates connections of direct lake semantic models which use a lakehouse. Warehouse could be easily supported also using Semantic Link Labs see <a href=https://semantic-link-labs.readthedocs.io/en/stable/sempy_labs.directlake.html#sempy_labs.directlake.update_direct_lake_model_connection/>this link</a>\n</li></ul>\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a98b6d0a-7a36-4116-ab0d-aa70144eb737"},{"cell_type":"markdown","source":["##### Set parameters\n","Before running this notebook ensure these parameters are set correctly. If necessary these can be passed in via a data factory pipeline"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"dee81614-b92b-4242-890a-b11f97b1a640"},{"cell_type":"code","source":["branch_name = '' #eg 'DEV_Custom_Feature_06_branch'\n","project_name='' #eg Fabric_CICD_Option3'\n","repo_name='' #eg 'Workspace_CICD_Option3'\n","main_branch = '' #eg 'main'\n","branch_to_new_ws = '' #eg 'Workspace_' + branch_name\n","FABRIC_API_URL = '' #eg \"https://api.fabric.microsoft.com/v1\"\n","ADO_API_URL = '' #eg \"https://dev.azure.com/MCAPS553100\"\n","\n","# Azure Devops Key Vault Name and Secret Name\n","nameOfKeyVault = '' #eg 'azdosc'\n","secret_name = '' #eg 'azdopat'\n","\n","# enter pattern match for creating shortcuts - see https://github.com/arasdk/fabric-code-samples/blob/main/shortcuts/fabric_shortcut_creator.py \n","PATTERN_MATCH = [\"*\"]\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"tags":["parameters"]},"id":"90efaa4f-846d-4924-900e-258837a3467d"},{"cell_type":"markdown","source":["##### Install semantic link labs to support advanced functionality\n","\n","https://semantic-link-labs.readthedocs.io/en/latest/index.html\n","https://github.com/microsoft/semantic-link-labs/blob/main/README.md\n","\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3b887bd6-a9c9-430f-b58f-b58a93f5ce29"},{"cell_type":"code","source":["%pip -q install semantic-link-labs"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":true},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1b03316d-c088-4a0e-a2f0-44d45d112121"},{"cell_type":"markdown","source":["##### Library imports and fabric rest client setup\n","\n","https://learn.microsoft.com/en-us/python/api/semantic-link-sempy/sempy.fabric.fabricrestclient"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4fb01e1d-ec4e-4c69-b544-66f6d8c5a475"},{"cell_type":"code","source":["import pandas as pd\n","import datetime\n","import re,json, fnmatch,os\n","import requests, base64\n","import sempy\n","import sempy.fabric as fabric\n","from sempy.fabric.exceptions import FabricHTTPException, WorkspaceNotFoundException\n","from pyspark.sql import DataFrame\n","from pyspark.sql.functions import col,current_timestamp,lit\n","import sempy_labs as labs\n","from sempy_labs import migration, directlake\n","from sempy_labs import lakehouse as lake\n","from sempy_labs import report as rep\n","from sempy_labs.tom import connect_semantic_model\n","\n","# instantiate the Fabric rest client\n","client = fabric.FabricRestClient()\n","\n","# get the current workspace ID based on the context of where this notebook is run from\n","thisWsId = notebookutils.runtime.context['currentWorkspaceId']\n","thisWsName = notebookutils.runtime.context['currentWorkspaceName']"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"391624c1-b299-452d-9ebf-f32626d49970"},{"cell_type":"markdown","source":["##### Get Workspace Git Connection Details"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c25c1e21-e58b-4d28-b748-3594cc823ce9"},{"cell_type":"code","source":["url = \"/v1/workspaces/\" + thisWsId + \"/git/connection\"\n","try:\n","    print(\"Retreiving git details for current workspace...\")\n","    response = client.get(url)\n","    gitProviderDetailsJSON = response.json()['gitProviderDetails']\n","except Exception as error:\n","    errmsg =  \"Couldn't get git connection status for current workspace.\"\n","    if (verbose):\n","            errmsg = errmsg + \"Error: \"+str(error)\n","    print(str(errmsg))\n","\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a8eebcaa-1262-4ecd-b914-3ea6665f9908"},{"cell_type":"markdown","source":["##### Create new AzDO branch based on main"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"326f2a72-42a2-4c84-b6a4-bf76348ce810"},{"cell_type":"code","source":["def get_branch_object_id(project_name, repo_name, branch_name, token):\n","    try:\n","        print(f\"Retriving ID of main branch {branch_name} to be cloned \")\n","        headers = {'Authorization': f'Basic {token}',\n","                    'Content-Type': 'application/json'\n","                    }\n","        #print(f\"{ADO_API_URL}/{project_name}/_apis/git/repositories/{repo_name}/refs/heads/{branch_name}?api-version=7.1\")\n","        response = requests.get(f\"{ADO_API_URL}/{project_name}/_apis/git/repositories/{repo_name}/refs/heads/{branch_name}?api-version=7.1\", headers=headers)\n","        return response.json()[\"value\"][0][\"objectId\"]\n","    except requests.exceptions.RequestException as e:\n","        print(f\"Error getting branch object ID: {e}\")\n","        return None\n","\n","def encode_pat(pat):\n","    # Encode the PAT in base64\n","    encoded_pat = base64.b64encode(pat.encode('utf-8')).decode('utf-8')\n","    return encoded_pat\n","\n","access_token =notebookutils.credentials.getToken('keyvault')\n","url = f'https://{nameOfKeyVault}.vault.azure.net/secrets/{secret_name}?api-version=7.3'\n","headers = {\n","    'Authorization': f'Bearer {access_token}',\n","    'Content-Type': 'application/json'\n","}\n","\n","response = requests.get(url, headers=headers)\n","if response.status_code == 200:\n","    #print(response.json()['value'])\n","    pat_token =  encode_pat(':'+response.json()['value'])\n","else:\n","    print(f\"Failed to get secret: {response.status_code} - {response.text}\")\n","\n","try:\n","    print(f\"Creating feature branch {branch_name} based on {main_branch} in progress\")\n","    headers = {\"Authorization\": f\"Basic {pat_token}\", \"Content-Type\": \"application/json\"}\n","    data =  [\n","                {\n","            \"name\":f\"refs/heads/{branch_name}\",\n","            \"oldObjectId\": \"0000000000000000000000000000000000000000\",\n","            \"newObjectId\": get_branch_object_id(project_name, repo_name, main_branch, pat_token)\n","            }\n","        ]\n","    response = requests.post(f\"{ADO_API_URL}/{project_name}/_apis/git/repositories/{repo_name}/refs?api-version=7.1\", headers=headers, json=data)\n","    response.raise_for_status()\n","    print(f\"Feature branch {branch_name} created\")\n","except requests.exceptions.RequestException as e:\n","    print(f\"Error creating Azure DevOps branch: {e}\")\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"editable":true,"run_control":{"frozen":false}},"id":"09cfe6e2-7dce-4331-b90f-b09fa2872808"},{"cell_type":"markdown","source":["##### Create new feature branch workspace"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a99c8ade-acee-4d63-8b30-a80e11dc092d"},{"cell_type":"code","source":["\n","try:\n","    # get current capacity ID\n","    response = client.get(f\"v1/workspaces/{thisWsId}\")\n","    current_capacity_id = response.json()['capacityId']\n","    # create new workspace\n","    print(\"Creating workspace: \" + branch_to_new_ws + \" in capacity \"+ current_capacity_id +\"...\")\n","    response = fabric.create_workspace(branch_to_new_ws,current_capacity_id) \n","    new_workspace_id = response\n","    print(\"Created workspace with ID: \" + new_workspace_id)\n","except Exception as error:\n","    errmsg =  \"Failed to recreate workspace \" +branch_to_new_ws + \" with capacity ID (\"+ current_capacity_id + \") due to: \"+str(error)\n","    print(errmsg)\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d643c26f-929f-4801-aebc-1fdfe86f2c73"},{"cell_type":"markdown","source":["##### Change the Git connection details to use new branch"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f96aa7cb-8eae-49d9-8fcb-60cc56cba787"},{"cell_type":"code","source":["gitpayloadstr= '{\"gitProviderDetails\": ' + json.dumps(gitProviderDetailsJSON) + '}'\n","print(\"Before: \" + gitpayloadstr)\n","gitpayload = json.loads(gitpayloadstr)\n","gitpayload[\"gitProviderDetails\"][\"branchName\"] = branch_name\n","\n","print(\"After: \" +json.dumps(gitpayload))"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6bc035a3-3611-4440-9a99-a1b27b171600"},{"cell_type":"markdown","source":["##### Connect new feature branch workspace to git and sync"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"08d041c1-f1dd-40c4-bb6b-b79e02f69029"},{"cell_type":"code","source":["\n","url = \"v1/workspaces/\" +  new_workspace_id + \"/git/connect\"\n","\n","try:\n","    print('Attempting to connect workspace '+ branch_to_new_ws)\n","    response = client.post(url,json= gitpayload)\n","    print(str(response.status_code) + response.text) \n","    success = True\n","    \n","except Exception as error:\n","    errmsg =  \"Couldn't connect git to workspace \" + branch_to_new_ws + \". Error: \"+str(error)\n","    print(str(errmsg))\n","    success = False\n","# If connection successful then try to initialise    \n","if (success):\n","    url = \"/v1/workspaces/\" + new_workspace_id + \"/git//initializeConnection\"\n","    payload = {\"initializationStrategy\":\"PreferRemote\"}\n","    try:\n","        print('Attempting to initialize git connection for workspace '+ branch_to_new_ws)\n","        response = client.post(url,json= payload)\n","        #print(str(response.status_code) + response.text) \n","        commithash = response.json()['remoteCommitHash']\n","        print('Successfully initialized. Updating with commithash '+commithash)\n","        if commithash!='':\n","            url = \"/v1/workspaces/\" + new_workspace_id + \"/git/updateFromGit\"\n","            payload = '{\"remoteCommitHash\": \"' + commithash + '\",\"conflictResolution\": {\"conflictResolutionType\": \"Workspace\",\"conflictResolutionPolicy\": \"PreferWorkspace\"},\"options\": {\"allowOverrideItems\": true}}'\n","            response = client.post(url,json= json.loads(payload))\n","            print(str(response.status_code))\n","    except Exception as error:\n","        errmsg =  \"Couldn't initialize git for workspace \" +branch_to_new_ws + \". Error: \"+str(error)\n","        print(str(errmsg))"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8704cc7c-30bd-4296-b5eb-d27e6f8c8d59"},{"cell_type":"code","source":["import time\n","# wait for git items to sync\n","time.sleep(30)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"822bac79-5bd9-4caa-9f97-7a6355e0f9f6"},{"cell_type":"markdown","source":["##### Update default lakehouses for notebooks and create shortcuts\n","\n","Update notebook dependencies:\n","https://github.com/PowerBiDevCamp/FabConWorkshopSweden/blob/main/DemoFiles/GitUpdateWorkspace/updateWorkspaceDependencies_v1.ipynb\n","\n","Shortcut creator:\n","https://github.com/arasdk/fabric-code-samples/blob/main/shortcuts/fabric_shortcut_creator.py "],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"aaae8a08-588d-4dd8-9d2c-2200b7a88d30"},{"cell_type":"code","source":["\n","# Extract workspace_id, item_id and path from a onelake URI\n","def extract_onelake_https_uri_components(uri):\n","    # Define a regular expression to match any string between slashes and capture the final path element(s) without the leading slash\n","    pattern = re.compile(r\"abfss://([^@]+)@[^/]+/([^/]+)/(.*)\")\n","    match = pattern.search(uri)\n","    if match:\n","        workspace_id, item_id, path = match.groups()\n","        return workspace_id, item_id, path\n","    else:\n","        return None, None, None\n","\n","\n","def is_valid_onelake_uri(uri: str) -> bool:\n","    workspace_id, item_id, path = extract_onelake_https_uri_components(uri)\n","    if \"abfss://\" not in uri or workspace_id is None or item_id is None or path is None:\n","        return False\n","\n","    return True\n","\n","\n","def get_last_path_segment(uri: str):\n","    path = uri.split(\"/\")  # Split the entire URI by '/'\n","    return path[-1] if path else None\n","\n","\n","def is_delta_table(uri: str):\n","    delta_log_path = os.path.join(uri, \"_delta_log\")\n","    return mssparkutils.fs.exists(delta_log_path)\n","\n","\n","def get_onelake_shorcut(workspace_id: str, item_id: str, path: str, name: str):\n","    shortcut_uri = (\n","        f\"v1/workspaces/{workspace_id}/items/{item_id}/shortcuts/{path}/{name}\"\n","    )\n","    result = client.get(shortcut_uri).json()\n","    return result\n","\n","\n","def is_folder_matching_pattern(path: str, folder_name: str, patterns: []):\n","    if folder_name in patterns:\n","        return True\n","    else:\n","        for pattern in patterns:\n","            if fnmatch.fnmatch(folder_name, pattern):\n","                return is_delta_table(path)\n","\n","    return False\n","\n","\n","def get_matching_delta_tables_uris(uri: str, patterns: []) -> []:\n","    # Use a set to avoid duplicates\n","    matched_uris = set()\n","    files = mssparkutils.fs.ls(uri)\n","    folders = [item for item in files if item.isDir]\n","\n","    # Filter folders to only those that matches the pattern and is a delta table\n","    matched_uris.update(\n","        folder.path\n","        for folder in folders\n","        if is_folder_matching_pattern(folder.path, folder.name, patterns)\n","    )\n","\n","    return matched_uris\n","\n","\n","def create_onelake_shorcut(source_uri: str, dest_uri: str):\n","    src_workspace_id, src_item_id, src_path = extract_onelake_https_uri_components(\n","        source_uri\n","    )\n","\n","    dest_workspace_id, dest_item_id, dest_path = extract_onelake_https_uri_components(\n","        dest_uri\n","    )\n","\n","    name = get_last_path_segment(source_uri)\n","    dest_uri_joined = os.path.join(dest_uri, name)\n","\n","    # If the destination path already exists, return without creating shortcut\n","    if mssparkutils.fs.exists(dest_uri_joined):\n","        print(f\"Destination already exists: {dest_uri_joined}\")\n","        return None\n","\n","    request_body = {\n","        \"name\": name,\n","        \"path\": dest_path,\n","        \"target\": {\n","            \"oneLake\": {\n","                \"itemId\": src_item_id,\n","                \"path\": src_path,\n","                \"workspaceId\": src_workspace_id,\n","            }\n","        },\n","    }\n","\n","    shortcut_uri = f\"v1/workspaces/{dest_workspace_id}/items/{dest_item_id}/shortcuts\"\n","    print(f\"Creating shortcut: {shortcut_uri}/{name}..\")\n","    try:\n","        client.post(shortcut_uri, json=request_body)\n","    except FabricHTTPException as e:\n","        print(e)\n","        return None\n","\n","    return get_onelake_shorcut(dest_workspace_id, dest_item_id, dest_path, name)\n","   \n","\n","for notebook in notebookutils.notebook.list(workspaceId=new_workspace_id):\n","    if notebook.displayName != 'Create Feature Branch':\n","\n","        # Get the current notebook definition\n","        notebook_def = notebookutils.notebook.getDefinition(notebook.displayName,workspaceId=new_workspace_id)\n","        json_payload = json.loads(notebook_def)\n","        \n","        # Check and remove any attached lakehouses\n","        if 'dependencies' in json_payload['metadata'] and 'lakehouse' in json_payload['metadata']['dependencies']:\n","        # Remove all lakehouses\n","            current_lakehouse = json_payload['metadata']['dependencies']['lakehouse']\n","            json_payload['metadata']['dependencies']['lakehouse'] = {}\n","\n","            #Update new notebook definition after removing existing lakehouses and with new default lakehouseId\n","            (notebookutils.notebook.updateDefinition(\n","                        name = notebook.displayName,\n","                        content  = json.dumps(json_payload),  \n","                        defaultLakehouse = current_lakehouse['default_lakehouse_name'],\n","                        defaultLakehouseWorkspace = new_workspace_id,\n","                        workspaceId = new_workspace_id\n","                        )\n","                )\n","            print(f\"Updated notebook {notebook.displayName} with new default lakehouse: {current_lakehouse['default_lakehouse_name']} in {new_workspace_id}\")\n","            # fetch ID of target lakehouse in new workspace\n","            source_lh_id = notebookutils.lakehouse.getWithProperties(name=current_lakehouse['default_lakehouse_name'], workspaceId=thisWsId)['id']\n","            target_lh_id = notebookutils.lakehouse.getWithProperties(name=current_lakehouse['default_lakehouse_name'], workspaceId=new_workspace_id)['id']\n","\n","            SOURCE_URI = f\"abfss://{thisWsId}@onelake.dfs.fabric.microsoft.com/{source_lh_id}/Tables\"\n","            DEST_URI = f\"abfss://{new_workspace_id}@onelake.dfs.fabric.microsoft.com/{target_lh_id}/Tables\"\n","\n","            if PATTERN_MATCH is None or len(PATTERN_MATCH) == 0:\n","                raise TypeError(\"Argument 'PATTERN_MATCH' should be a valid list of patterns or [\"*\"] to match everything\")\n","\n","            # Collect created shortcuts\n","            result = []\n","\n","            # If either URI's are invalid, just return\n","            if not is_valid_onelake_uri(SOURCE_URI) or not is_valid_onelake_uri(DEST_URI):\n","                print(\n","                    \"invalid URI's provided. URI's should be in the form: abfss://<workspace-id>@onelake.dfs.fabric.microsoft.com/<item-id>/<path>\"\n","                )\n","            else:\n","                # Remove any trailing '/' from uri's\n","                source_uri_addr = SOURCE_URI.rstrip(\"/\")\n","                dest_uri_addr = DEST_URI.rstrip(\"/\")\n","\n","                dest_workspace_id, dest_item_id, dest_path = extract_onelake_https_uri_components(\n","                    dest_uri_addr\n","                )\n","\n","                # If we are not shortcutting to a managed table folder or\n","                # the source uri is a delta table, just shortcut it 1-1.\n","                if not dest_path.startswith(\"Tables\") or is_delta_table(source_uri_addr):\n","                    shortcut = create_onelake_shorcut(source_uri_addr, dest_uri_addr)\n","                    if shortcut is not None:\n","                        result.append(shortcut)\n","                else:\n","                    # If source is not a delta table, and destination is managed table folder:\n","                    # Iterate over source folders and create table shortcuts @ destination\n","                    for delta_table_uri in get_matching_delta_tables_uris(\n","                        source_uri_addr, PATTERN_MATCH\n","                    ):\n","                        shortcut = create_onelake_shorcut(delta_table_uri, dest_uri_addr)\n","                        if shortcut is not None:\n","                            result.append(shortcut)\n","            print(result)\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5c60b5d2-f83c-46f8-9870-9fd609166b67"},{"cell_type":"markdown","source":["###### Update direct lake model lakehouse connection\n","\n","https://semantic-link-labs.readthedocs.io/en/stable/sempy_labs.directlake.html#sempy_labs.directlake.update_direct_lake_model_lakehouse_connection\n","    "],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cc97be77-116e-4cde-bdc6-2971ab98a083"},{"cell_type":"code","source":["\n","df_datasets = fabric.list_datasets(branch_to_new_ws)\n","\n","# Iterate over each dataset in the dataframe\n","for index, row in df_datasets.iterrows():\n","    # Check if the dataset is not the default semantic model\n","    if not labs.is_default_semantic_model(row['Dataset Name'], fabric.resolve_workspace_id(branch_to_new_ws)):\n","        print('Updating semantic model connection ' + row['Dataset Name'])\n","        labs.directlake.update_direct_lake_model_lakehouse_connection(dataset=row['Dataset Name'], workspace= branch_to_new_ws,lakehouse =labs.directlake.get_direct_lake_source(row['Dataset Name'], workspace= branch_to_new_ws)[1], lakehouse_workspace=branch_to_new_ws)\n","        labs.refresh_semantic_model(dataset=row['Dataset Name'], workspace= branch_to_new_ws)\n","\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9deccda6-5c3d-4b88-8ed8-68855ca0949a"},{"cell_type":"markdown","source":["##### Rebind reports in new branch workspace\n","\n","https://semantic-link-labs.readthedocs.io/en/latest/sempy_labs.report.html#sempy_labs.report.report_rebind"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d68b575f-3692-4bd8-b156-e8f18596213a"},{"cell_type":"code","source":["df_reports = fabric.list_reports(workspace=branch_to_new_ws)\n","for index, row in df_reports.iterrows():\n","    #print(row['Name'] + '-' + row['Dataset Id'])\n","    df_datasets = fabric.list_datasets(workspace=branch_to_new_ws)\n","    dataset_name = df_datasets[df_datasets['Dataset ID'] == row['Dataset Id']]['Dataset Name'].values[0]\n","    print(dataset_name)\n","    labs.report.report_rebind(report=row['Name'],dataset=dataset_name, report_workspace=branch_to_new_ws, dataset_workspace=branch_to_new_ws)\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"06268ede-b795-493e-9a8d-772654ce7e20"},{"cell_type":"markdown","source":["###### Commit changes made above to Git"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d6b55196-0faf-431b-b6dc-715083fcaad5"},{"cell_type":"code","source":["json_body = {\n","\t\"mode\": \"All\",\n","    \"comment\": \"Update datasets connections. Rebind reports.\"\n","}\n","\n","resp = client.post(f\"v1/workspaces/{new_workspace_id}/git/commitToGit\",json=json_body)\n","print(resp)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b3d3260f-ffb8-402a-8c5e-6a804624557f"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{}}},"nbformat":4,"nbformat_minor":5}